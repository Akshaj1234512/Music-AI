{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guitar Transcription Pipeline - Component-by-Component Debugging\n",
    "\n",
    "This notebook breaks down the 4-stage embedding pipeline into individual components for detailed inspection and debugging.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. **Basic Pitch** - Spotify pretrained model (440 dims)\n",
    "2. **Meta Encodec** - Audio compression (128 dims)\n",
    "3. **VQ-VAE** - Discrete representation (64 dims + tokens)\n",
    "4. **CLAP** - Semantic understanding (768 dims)\n",
    "5. **Fusion** - Combined embeddings (768 dims)\n",
    "6. **Audio Decoder** - Transcription (88 piano keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Import our models\nimport sys\nsys.path.append('..')\n\nfrom models.basic_pitch_wrapper import BasicPitchFeatureExtractor\nfrom models.huggingface_encodec import HuggingFaceEncodec\nfrom models.vq_vae import KenaVQVAE\nfrom models.huggingface_clap import HuggingFaceCLAP\nfrom models.embedding_validation_decoder import EmbeddingValidationDecoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Input Audio\n",
    "\n",
    "Create a synthetic guitar-like audio signal for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_guitar_audio(duration=3.0, sample_rate=22050, seed=42):\n",
    "    \"\"\"\n",
    "    Create synthetic guitar audio with multiple harmonics and envelope.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = np.linspace(0, duration, int(duration * sample_rate))\n",
    "    \n",
    "    # Guitar chord: Em (E2, G3, B3, E4)\n",
    "    frequencies = [82.41, 196.0, 246.94, 329.63]  # E2, G3, B3, E4\n",
    "    audio = np.zeros_like(t)\n",
    "    \n",
    "    for i, freq in enumerate(frequencies):\n",
    "        # Fundamental + harmonics\n",
    "        fundamental = np.sin(2 * np.pi * freq * t)\n",
    "        harmonic2 = 0.3 * np.sin(2 * np.pi * 2 * freq * t)\n",
    "        harmonic3 = 0.1 * np.sin(2 * np.pi * 3 * freq * t)\n",
    "        \n",
    "        # String-specific envelope (different decay rates)\n",
    "        decay_rate = 0.5 + i * 0.2  # Different decay for each string\n",
    "        envelope = np.exp(-t * decay_rate)\n",
    "        \n",
    "        # Add some vibrato\n",
    "        vibrato = 1 + 0.02 * np.sin(2 * np.pi * 5 * t)\n",
    "        \n",
    "        string_signal = (fundamental + harmonic2 + harmonic3) * envelope * vibrato\n",
    "        audio += string_signal * (0.8 - i * 0.15)  # Different volumes\n",
    "    \n",
    "    # Add some noise for realism\n",
    "    noise = 0.02 * np.random.randn(len(audio))\n",
    "    audio += noise\n",
    "    \n",
    "    # Normalize\n",
    "    audio = audio / np.max(np.abs(audio)) * 0.7\n",
    "    \n",
    "    return torch.tensor(audio, dtype=torch.float32)\n",
    "\n",
    "# Create synthetic audio\n",
    "audio_length = 3.0  # seconds\n",
    "sample_rate = 22050\n",
    "synthetic_audio = create_synthetic_guitar_audio(duration=audio_length, sample_rate=sample_rate)\n",
    "\n",
    "print(f\"Generated synthetic audio:\")\n",
    "print(f\"  Shape: {synthetic_audio.shape}\")\n",
    "print(f\"  Duration: {len(synthetic_audio) / sample_rate:.2f} seconds\")\n",
    "print(f\"  Sample rate: {sample_rate} Hz\")\n",
    "print(f\"  Range: [{synthetic_audio.min():.3f}, {synthetic_audio.max():.3f}]\")\n",
    "\n",
    "# Plot the audio waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "time_axis = np.linspace(0, audio_length, len(synthetic_audio))\n",
    "plt.plot(time_axis, synthetic_audio.numpy())\n",
    "plt.title('Synthetic Guitar Audio (Em Chord)')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add batch dimension for processing\n",
    "audio_batch = synthetic_audio.unsqueeze(0)  # [1, time]\n",
    "print(f\"\\nBatched audio shape: {audio_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Basic Pitch Feature Extraction\n",
    "\n",
    "Extract features using the real Spotify Basic Pitch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 1: BASIC PITCH FEATURE EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize Basic Pitch\n",
    "try:\n",
    "    basic_pitch = BasicPitchFeatureExtractor(\n",
    "        sample_rate=sample_rate,\n",
    "        device='cpu'  # Use CPU for debugging\n",
    "    )\n",
    "    print(\"âœ… Basic Pitch initialized successfully\")\n",
    "    print(f\"   Output dimension: {basic_pitch.output_dim}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Basic Pitch initialization failed: {e}\")\n",
    "    basic_pitch = None\n",
    "\n",
    "if basic_pitch is not None:\n",
    "    # Extract features\n",
    "    print(\"\\nðŸ”„ Extracting Basic Pitch features...\")\n",
    "    try:\n",
    "        pitch_features = basic_pitch(audio_batch)\n",
    "        print(f\"âœ… Features extracted successfully\")\n",
    "        print(f\"   Shape: {pitch_features.shape}\")\n",
    "        print(f\"   Data type: {pitch_features.dtype}\")\n",
    "        print(f\"   Range: [{pitch_features.min():.3f}, {pitch_features.max():.3f}]\")\n",
    "        print(f\"   Mean: {pitch_features.mean():.3f}\")\n",
    "        print(f\"   Std: {pitch_features.std():.3f}\")\n",
    "        \n",
    "        # Check for reasonable values\n",
    "        has_nan = torch.isnan(pitch_features).any()\n",
    "        has_inf = torch.isinf(pitch_features).any()\n",
    "        print(f\"   Contains NaN: {has_nan}\")\n",
    "        print(f\"   Contains Inf: {has_inf}\")\n",
    "        \n",
    "        # Visualize features\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Plot feature heatmap (first 100 features for visibility)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        features_to_show = pitch_features[0, :, :100].detach().numpy()\n",
    "        plt.imshow(features_to_show.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('Basic Pitch Features (First 100 dims)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Feature Dimension')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot feature statistics over time\n",
    "        plt.subplot(2, 2, 2)\n",
    "        frame_means = pitch_features[0].mean(dim=1).detach().numpy()\n",
    "        frame_stds = pitch_features[0].std(dim=1).detach().numpy()\n",
    "        frames = np.arange(len(frame_means))\n",
    "        plt.plot(frames, frame_means, label='Mean', alpha=0.7)\n",
    "        plt.plot(frames, frame_stds, label='Std', alpha=0.7)\n",
    "        plt.title('Feature Statistics Over Time')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot feature distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        all_features = pitch_features[0].flatten().detach().numpy()\n",
    "        plt.hist(all_features, bins=50, alpha=0.7, density=True)\n",
    "        plt.title('Feature Value Distribution')\n",
    "        plt.xlabel('Feature Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot activation patterns\n",
    "        plt.subplot(2, 2, 4)\n",
    "        activation_sum = pitch_features[0].sum(dim=0).detach().numpy()\n",
    "        plt.plot(activation_sum[:88], label='Onset (0-87)', alpha=0.7)\n",
    "        plt.plot(activation_sum[88:88+88], label='Note (88-175)', alpha=0.7)\n",
    "        plt.plot(activation_sum[88+88:88+88+64], label='Contour (176-239)', alpha=0.7)\n",
    "        plt.title('Activation Patterns by Output Type')\n",
    "        plt.xlabel('Pitch/Feature Index')\n",
    "        plt.ylabel('Total Activation')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Feature extraction failed: {e}\")\n",
    "        pitch_features = None\n",
    "else:\n",
    "    pitch_features = None\n",
    "    print(\"âš ï¸ Skipping Basic Pitch - using random features for testing\")\n",
    "    pitch_features = torch.randn(1, 129, 440)  # Approximate expected shape\n",
    "    print(f\"   Random features shape: {pitch_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Meta Encodec Audio Compression\n",
    "\n",
    "Compress audio using Meta's Encodec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 2: META ENCODEC COMPRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize Encodec\n",
    "try:\n",
    "    encodec = MetaEncodecWrapper(\n",
    "        bandwidth=6.0,  # kbps\n",
    "        sample_rate=sample_rate\n",
    "    )\n",
    "    print(\"âœ… Encodec initialized successfully\")\n",
    "    print(f\"   Output dimension: {encodec.output_dim}\")\n",
    "    print(f\"   Bandwidth: 6.0 kbps\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Encodec initialization failed: {e}\")\n",
    "    encodec = None\n",
    "\n",
    "if encodec is not None:\n",
    "    # Compress audio\n",
    "    print(\"\\nðŸ”„ Compressing audio with Encodec...\")\n",
    "    try:\n",
    "        encodec_codes, encodec_embeddings = encodec(audio_batch)\n",
    "        print(f\"âœ… Audio compressed successfully\")\n",
    "        print(f\"   Codes shape: {encodec_codes.shape}\")\n",
    "        print(f\"   Embeddings shape: {encodec_embeddings.shape}\")\n",
    "        print(f\"   Embeddings range: [{encodec_embeddings.min():.3f}, {encodec_embeddings.max():.3f}]\")\n",
    "        print(f\"   Embeddings mean: {encodec_embeddings.mean():.3f}\")\n",
    "        print(f\"   Embeddings std: {encodec_embeddings.std():.3f}\")\n",
    "        \n",
    "        # Check codes statistics\n",
    "        print(f\"   Codes range: [{encodec_codes.min()}, {encodec_codes.max()}]\")\n",
    "        print(f\"   Unique codes: {torch.unique(encodec_codes).numel()}\")\n",
    "        \n",
    "        # Visualize compression results\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot embedding heatmap\n",
    "        plt.subplot(3, 2, 1)\n",
    "        embeddings_viz = encodec_embeddings[0].detach().numpy()\n",
    "        plt.imshow(embeddings_viz.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('Encodec Embeddings')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Embedding Dimension')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot codes for each RVQ stage\n",
    "        plt.subplot(3, 2, 2)\n",
    "        codes_viz = encodec_codes[0].detach().numpy()\n",
    "        plt.imshow(codes_viz.T, aspect='auto', origin='lower', cmap='tab10')\n",
    "        plt.title('Encodec Codes (8 RVQ Stages)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('RVQ Stage')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot embedding statistics over time\n",
    "        plt.subplot(3, 2, 3)\n",
    "        emb_means = encodec_embeddings[0].mean(dim=1).detach().numpy()\n",
    "        emb_stds = encodec_embeddings[0].std(dim=1).detach().numpy()\n",
    "        frames = np.arange(len(emb_means))\n",
    "        plt.plot(frames, emb_means, label='Mean', alpha=0.7)\n",
    "        plt.plot(frames, emb_stds, label='Std', alpha=0.7)\n",
    "        plt.title('Embedding Statistics Over Time')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot code usage histogram\n",
    "        plt.subplot(3, 2, 4)\n",
    "        all_codes = encodec_codes.flatten().detach().numpy()\n",
    "        plt.hist(all_codes, bins=50, alpha=0.7)\n",
    "        plt.title('Codebook Usage Distribution')\n",
    "        plt.xlabel('Code Index')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot embedding distribution\n",
    "        plt.subplot(3, 2, 5)\n",
    "        all_embeddings = encodec_embeddings.flatten().detach().numpy()\n",
    "        plt.hist(all_embeddings, bins=50, alpha=0.7, density=True)\n",
    "        plt.title('Embedding Value Distribution')\n",
    "        plt.xlabel('Embedding Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot compression ratio info\n",
    "        plt.subplot(3, 2, 6)\n",
    "        original_size = audio_batch.numel() * 4  # 4 bytes per float32\n",
    "        compressed_size = encodec_codes.numel() * 2  # Assume 2 bytes per code\n",
    "        compression_ratio = original_size / compressed_size\n",
    "        \n",
    "        plt.bar(['Original', 'Compressed'], [original_size/1024, compressed_size/1024])\n",
    "        plt.title(f'Compression Ratio: {compression_ratio:.1f}x')\n",
    "        plt.ylabel('Size (KB)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Encodec compression failed: {e}\")\n",
    "        encodec_codes = None\n",
    "        encodec_embeddings = None\n",
    "else:\n",
    "    encodec_codes = None\n",
    "    encodec_embeddings = None\n",
    "    print(\"âš ï¸ Skipping Encodec - using random codes/embeddings for testing\")\n",
    "    # Approximate expected shapes\n",
    "    encodec_codes = torch.randint(0, 1024, (1, 129, 8))  # [batch, time, 8_stages]\n",
    "    encodec_embeddings = torch.randn(1, 129, 128)  # [batch, time, 128]\n",
    "    print(f\"   Random codes shape: {encodec_codes.shape}\")\n",
    "    print(f\"   Random embeddings shape: {encodec_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Kena VQ-VAE Discrete Representation\n",
    "\n",
    "Process features through VQ-VAE to get discrete tokens and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 3: KENA VQ-VAE DISCRETE REPRESENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize VQ-VAE\n",
    "try:\n",
    "    vq_vae = KenaVQVAE(\n",
    "        input_dim=440,  # From Basic Pitch\n",
    "        codebook_size=512,\n",
    "        codebook_dim=64,\n",
    "        hidden_dims=[512, 512, 512, 512]\n",
    "    )\n",
    "    print(\"âœ… VQ-VAE initialized successfully\")\n",
    "    print(f\"   Input dimension: 440\")\n",
    "    print(f\"   Codebook size: 512\")\n",
    "    print(f\"   Codebook dimension: 64\")\n",
    "    print(f\"   Hidden dimensions: [512, 512, 512, 512]\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ VQ-VAE initialization failed: {e}\")\n",
    "    vq_vae = None\n",
    "\n",
    "if vq_vae is not None and pitch_features is not None:\n",
    "    # Process through VQ-VAE\n",
    "    print(\"\\nðŸ”„ Processing through VQ-VAE...\")\n",
    "    try:\n",
    "        vq_vae.eval()  # Set to eval mode for debugging\n",
    "        with torch.no_grad():\n",
    "            vq_outputs = vq_vae(pitch_features)\n",
    "        \n",
    "        vq_embeddings = vq_outputs['z_q']\n",
    "        vq_indices = vq_outputs['indices']\n",
    "        commitment_loss = vq_outputs['commitment_loss']\n",
    "        \n",
    "        print(f\"âœ… VQ-VAE processing successful\")\n",
    "        print(f\"   Embeddings shape: {vq_embeddings.shape}\")\n",
    "        print(f\"   Indices shape: {vq_indices.shape}\")\n",
    "        print(f\"   Commitment loss: {commitment_loss.item():.4f}\")\n",
    "        print(f\"   Embeddings range: [{vq_embeddings.min():.3f}, {vq_embeddings.max():.3f}]\")\n",
    "        print(f\"   Embeddings mean: {vq_embeddings.mean():.3f}\")\n",
    "        print(f\"   Embeddings std: {vq_embeddings.std():.3f}\")\n",
    "        print(f\"   Unique tokens: {torch.unique(vq_indices).numel()}/{512}\")\n",
    "        print(f\"   Token range: [{vq_indices.min()}, {vq_indices.max()}]\")\n",
    "        \n",
    "        # Visualize VQ-VAE results\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        # Plot VQ embeddings heatmap\n",
    "        plt.subplot(3, 3, 1)\n",
    "        vq_viz = vq_embeddings[0].detach().numpy()\n",
    "        plt.imshow(vq_viz.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('VQ-VAE Embeddings (64 dims)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Embedding Dimension')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot discrete tokens\n",
    "        plt.subplot(3, 3, 2)\n",
    "        tokens_viz = vq_indices[0].detach().numpy()\n",
    "        plt.plot(tokens_viz, 'o-', alpha=0.7, markersize=3)\n",
    "        plt.title('Discrete Token Sequence')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Token Index')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot token usage histogram\n",
    "        plt.subplot(3, 3, 3)\n",
    "        unique_tokens, token_counts = torch.unique(vq_indices, return_counts=True)\n",
    "        plt.bar(unique_tokens.numpy(), token_counts.numpy(), alpha=0.7)\n",
    "        plt.title('Token Usage Distribution')\n",
    "        plt.xlabel('Token Index')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot embedding statistics over time\n",
    "        plt.subplot(3, 3, 4)\n",
    "        vq_means = vq_embeddings[0].mean(dim=1).detach().numpy()\n",
    "        vq_stds = vq_embeddings[0].std(dim=1).detach().numpy()\n",
    "        frames = np.arange(len(vq_means))\n",
    "        plt.plot(frames, vq_means, label='Mean', alpha=0.7)\n",
    "        plt.plot(frames, vq_stds, label='Std', alpha=0.7)\n",
    "        plt.title('VQ Embedding Stats Over Time')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot embedding value distribution\n",
    "        plt.subplot(3, 3, 5)\n",
    "        all_vq_embeddings = vq_embeddings.flatten().detach().numpy()\n",
    "        plt.hist(all_vq_embeddings, bins=50, alpha=0.7, density=True)\n",
    "        plt.title('VQ Embedding Distribution')\n",
    "        plt.xlabel('Embedding Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot codebook utilization\n",
    "        plt.subplot(3, 3, 6)\n",
    "        codebook_usage = torch.zeros(512)\n",
    "        for token in torch.unique(vq_indices):\n",
    "            codebook_usage[token] = (vq_indices == token).sum()\n",
    "        used_codes = (codebook_usage > 0).sum().item()\n",
    "        plt.bar(range(512), codebook_usage.numpy(), alpha=0.7)\n",
    "        plt.title(f'Codebook Utilization ({used_codes}/512 codes used)')\n",
    "        plt.xlabel('Codebook Index')\n",
    "        plt.ylabel('Usage Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot commitment loss info\n",
    "        plt.subplot(3, 3, 7)\n",
    "        plt.bar(['Commitment Loss'], [commitment_loss.item()], alpha=0.7)\n",
    "        plt.title('VQ-VAE Training Signal')\n",
    "        plt.ylabel('Loss Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot embedding dimension correlations (sample)\n",
    "        plt.subplot(3, 3, 8)\n",
    "        # Compute correlation matrix for first 16 dimensions\n",
    "        sample_dims = vq_embeddings[0, :, :16].detach().numpy().T\n",
    "        corr_matrix = np.corrcoef(sample_dims)\n",
    "        plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Embedding Dim Correlations (First 16)')\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.ylabel('Dimension')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot token transition patterns\n",
    "        plt.subplot(3, 3, 9)\n",
    "        token_seq = vq_indices[0].detach().numpy()\n",
    "        transitions = [(token_seq[i], token_seq[i+1]) for i in range(len(token_seq)-1)]\n",
    "        from collections import Counter\n",
    "        transition_counts = Counter(transitions)\n",
    "        most_common = transition_counts.most_common(10)\n",
    "        if most_common:\n",
    "            trans_labels = [f\"{t[0][0]}â†’{t[0][1]}\" for t in most_common]\n",
    "            trans_counts = [t[1] for t in most_common]\n",
    "            plt.barh(range(len(trans_labels)), trans_counts, alpha=0.7)\n",
    "            plt.yticks(range(len(trans_labels)), trans_labels)\n",
    "            plt.title('Most Common Token Transitions')\n",
    "            plt.xlabel('Frequency')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No transitions\\n(single frame?)', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Token Transitions')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ VQ-VAE processing failed: {e}\")\n",
    "        vq_embeddings = None\n",
    "        vq_indices = None\n",
    "        commitment_loss = None\n",
    "else:\n",
    "    vq_embeddings = None\n",
    "    vq_indices = None\n",
    "    commitment_loss = None\n",
    "    print(\"âš ï¸ Skipping VQ-VAE - using random embeddings/indices for testing\")\n",
    "    vq_embeddings = torch.randn(1, 129, 64)\n",
    "    vq_indices = torch.randint(0, 512, (1, 129))\n",
    "    commitment_loss = torch.tensor(0.1)\n",
    "    print(f\"   Random VQ embeddings shape: {vq_embeddings.shape}\")\n",
    "    print(f\"   Random VQ indices shape: {vq_indices.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: CLAP Semantic Understanding\n",
    "\n",
    "Extract semantic embeddings using HuggingFace CLAP music model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 4: CLAP SEMANTIC UNDERSTANDING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize CLAP\n",
    "try:\n",
    "    clap_encoder = HuggingFaceCLAP(\n",
    "        model_name=\"laion/larger_clap_music\",\n",
    "        output_dim=768,\n",
    "        freeze_model=True\n",
    "    )\n",
    "    print(\"âœ… CLAP initialized successfully\")\n",
    "    print(f\"   Model: laion/larger_clap_music\")\n",
    "    print(f\"   Output dimension: 768\")\n",
    "    print(f\"   Model frozen: True\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ CLAP initialization failed: {e}\")\n",
    "    clap_encoder = None\n",
    "\n",
    "if clap_encoder is not None:\n",
    "    # Extract semantic embeddings\n",
    "    print(\"\\nðŸ”„ Extracting CLAP semantic embeddings...\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            clap_embeddings = clap_encoder(audio_batch)\n",
    "        \n",
    "        print(f\"âœ… CLAP embeddings extracted successfully\")\n",
    "        print(f\"   Shape: {clap_embeddings.shape}\")\n",
    "        print(f\"   Range: [{clap_embeddings.min():.3f}, {clap_embeddings.max():.3f}]\")\n",
    "        print(f\"   Mean: {clap_embeddings.mean():.3f}\")\n",
    "        print(f\"   Std: {clap_embeddings.std():.3f}\")\n",
    "        print(f\"   L2 norm: {torch.norm(clap_embeddings, p=2, dim=-1).item():.3f}\")\n",
    "        \n",
    "        # Test zero-shot classification\n",
    "        print(\"\\nðŸŽµ Testing zero-shot audio classification...\")\n",
    "        guitar_labels = [\n",
    "            \"guitar chord\",\n",
    "            \"piano music\",\n",
    "            \"electric guitar\",\n",
    "            \"acoustic guitar\",\n",
    "            \"violin music\",\n",
    "            \"guitar slide technique\",\n",
    "            \"clean guitar playing\",\n",
    "            \"distorted guitar\"\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            classification_results = clap_encoder.classify_audio(audio_batch[0], guitar_labels)\n",
    "            print(\"   Classification results:\")\n",
    "            sorted_results = sorted(classification_results.items(), key=lambda x: x[1], reverse=True)\n",
    "            for label, score in sorted_results:\n",
    "                print(f\"     {label}: {score:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Classification failed: {e}\")\n",
    "        \n",
    "        # Visualize CLAP embeddings\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot embedding values\n",
    "        plt.subplot(2, 3, 1)\n",
    "        emb_values = clap_embeddings[0].detach().numpy()\n",
    "        plt.plot(emb_values, alpha=0.7)\n",
    "        plt.title('CLAP Embedding Values')\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot embedding distribution\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.hist(emb_values, bins=50, alpha=0.7, density=True)\n",
    "        plt.title('CLAP Embedding Distribution')\n",
    "        plt.xlabel('Embedding Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot top activated dimensions\n",
    "        plt.subplot(2, 3, 3)\n",
    "        abs_values = np.abs(emb_values)\n",
    "        top_dims = np.argsort(abs_values)[-20:]  # Top 20 dimensions\n",
    "        plt.bar(range(len(top_dims)), abs_values[top_dims], alpha=0.7)\n",
    "        plt.title('Top 20 Activated Dimensions')\n",
    "        plt.xlabel('Dimension Rank')\n",
    "        plt.ylabel('Absolute Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot L2 norm info\n",
    "        plt.subplot(2, 3, 4)\n",
    "        l2_norm = torch.norm(clap_embeddings, p=2, dim=-1).item()\n",
    "        plt.bar(['L2 Norm'], [l2_norm], alpha=0.7)\n",
    "        plt.title('Embedding L2 Norm (Should be ~1.0)')\n",
    "        plt.ylabel('Norm Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot classification scores if available\n",
    "        plt.subplot(2, 3, 5)\n",
    "        if 'classification_results' in locals() and classification_results:\n",
    "            labels, scores = zip(*sorted_results[:6])  # Top 6 results\n",
    "            plt.barh(range(len(labels)), scores, alpha=0.7)\n",
    "            plt.yticks(range(len(labels)), [l[:15] + '...' if len(l) > 15 else l for l in labels])\n",
    "            plt.title('Top Classification Scores')\n",
    "            plt.xlabel('Confidence')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Classification\\nNot Available', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            plt.title('Classification Scores')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot embedding sparsity\n",
    "        plt.subplot(2, 3, 6)\n",
    "        sparsity_levels = [0.01, 0.05, 0.1, 0.2]\n",
    "        sparsity_counts = [(np.abs(emb_values) < level).sum() for level in sparsity_levels]\n",
    "        sparsity_ratios = [count / len(emb_values) for count in sparsity_counts]\n",
    "        plt.bar([f'<{level}' for level in sparsity_levels], sparsity_ratios, alpha=0.7)\n",
    "        plt.title('Embedding Sparsity Analysis')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Fraction of Dimensions')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CLAP processing failed: {e}\")\n",
    "        clap_embeddings = None\n",
    "else:\n",
    "    clap_embeddings = None\n",
    "    print(\"âš ï¸ Skipping CLAP - using random embeddings for testing\")\n",
    "    clap_embeddings = torch.randn(1, 768)\n",
    "    # Normalize to simulate CLAP's normalized output\n",
    "    clap_embeddings = torch.nn.functional.normalize(clap_embeddings, p=2, dim=-1)\n",
    "    print(f\"   Random CLAP embeddings shape: {clap_embeddings.shape}\")\n",
    "    print(f\"   Random embeddings L2 norm: {torch.norm(clap_embeddings, p=2, dim=-1).item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Embedding Fusion\n",
    "\n",
    "Combine all embeddings into unified 768-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 5: EMBEDDING FUSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fusion parameters (from pipeline)\n",
    "embedding_dim = 768\n",
    "fusion_weights = {\n",
    "    'pitch': 0.4,\n",
    "    'encodec': 0.2,\n",
    "    'vq': 0.3,\n",
    "    'semantic': 0.1\n",
    "}\n",
    "\n",
    "print(f\"Target embedding dimension: {embedding_dim}\")\n",
    "print(f\"Fusion weights: {fusion_weights}\")\n",
    "\n",
    "# Create projection layers (simplified version of pipeline logic)\n",
    "pitch_proj = nn.Linear(440, embedding_dim) if pitch_features is not None else None\n",
    "encodec_proj = nn.Linear(128, embedding_dim) if encodec_embeddings is not None else None\n",
    "vq_proj = nn.Linear(64, embedding_dim) if vq_embeddings is not None else None\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "print(\"\\nðŸ”„ Fusing embeddings...\")\n",
    "\n",
    "try:\n",
    "    # Get the expected time dimension (use Basic Pitch as reference)\n",
    "    if pitch_features is not None:\n",
    "        target_time_frames = pitch_features.shape[1]\n",
    "    elif vq_embeddings is not None:\n",
    "        target_time_frames = vq_embeddings.shape[1]\n",
    "    else:\n",
    "        target_time_frames = 129  # Default\n",
    "    \n",
    "    print(f\"   Target time frames: {target_time_frames}\")\n",
    "    \n",
    "    # Project each component to embedding dimension\n",
    "    projected_components = {}\n",
    "    \n",
    "    # 1. Basic Pitch projection\n",
    "    if pitch_features is not None and pitch_proj is not None:\n",
    "        pitch_embedded = pitch_proj(pitch_features)\n",
    "        projected_components['pitch'] = pitch_embedded\n",
    "        print(f\"   âœ… Pitch projected: {pitch_features.shape} â†’ {pitch_embedded.shape}\")\n",
    "    else:\n",
    "        pitch_embedded = torch.zeros(1, target_time_frames, embedding_dim)\n",
    "        projected_components['pitch'] = pitch_embedded\n",
    "        print(f\"   âš ï¸ Pitch using zeros: {pitch_embedded.shape}\")\n",
    "    \n",
    "    # 2. Encodec projection (with temporal alignment)\n",
    "    if encodec_embeddings is not None and encodec_proj is not None:\n",
    "        # Temporal alignment if needed\n",
    "        if encodec_embeddings.shape[1] != target_time_frames:\n",
    "            encodec_aligned = torch.nn.functional.interpolate(\n",
    "                encodec_embeddings.transpose(1, 2),\n",
    "                size=target_time_frames,\n",
    "                mode='linear',\n",
    "                align_corners=False\n",
    "            ).transpose(1, 2)\n",
    "            print(f\"   ðŸ”„ Encodec aligned: {encodec_embeddings.shape[1]} â†’ {target_time_frames} frames\")\n",
    "        else:\n",
    "            encodec_aligned = encodec_embeddings\n",
    "        \n",
    "        encodec_embedded = encodec_proj(encodec_aligned)\n",
    "        projected_components['encodec'] = encodec_embedded\n",
    "        print(f\"   âœ… Encodec projected: {encodec_aligned.shape} â†’ {encodec_embedded.shape}\")\n",
    "    else:\n",
    "        encodec_embedded = torch.zeros(1, target_time_frames, embedding_dim)\n",
    "        projected_components['encodec'] = encodec_embedded\n",
    "        print(f\"   âš ï¸ Encodec using zeros: {encodec_embedded.shape}\")\n",
    "    \n",
    "    # 3. VQ-VAE projection (with temporal alignment)\n",
    "    if vq_embeddings is not None and vq_proj is not None:\n",
    "        # Temporal alignment if needed\n",
    "        if vq_embeddings.shape[1] != target_time_frames:\n",
    "            vq_aligned = torch.nn.functional.interpolate(\n",
    "                vq_embeddings.transpose(1, 2),\n",
    "                size=target_time_frames,\n",
    "                mode='linear',\n",
    "                align_corners=False\n",
    "            ).transpose(1, 2)\n",
    "            print(f\"   ðŸ”„ VQ-VAE aligned: {vq_embeddings.shape[1]} â†’ {target_time_frames} frames\")\n",
    "        else:\n",
    "            vq_aligned = vq_embeddings\n",
    "        \n",
    "        vq_embedded = vq_proj(vq_aligned)\n",
    "        projected_components['vq'] = vq_embedded\n",
    "        print(f\"   âœ… VQ-VAE projected: {vq_aligned.shape} â†’ {vq_embedded.shape}\")\n",
    "    else:\n",
    "        vq_embedded = torch.zeros(1, target_time_frames, embedding_dim)\n",
    "        projected_components['vq'] = vq_embedded\n",
    "        print(f\"   âš ï¸ VQ-VAE using zeros: {vq_embedded.shape}\")\n",
    "    \n",
    "    # 4. CLAP expansion (broadcast to time dimension)\n",
    "    if clap_embeddings is not None:\n",
    "        clap_expanded = clap_embeddings.unsqueeze(1).repeat(1, target_time_frames, 1)\n",
    "        projected_components['semantic'] = clap_expanded\n",
    "        print(f\"   âœ… CLAP expanded: {clap_embeddings.shape} â†’ {clap_expanded.shape}\")\n",
    "    else:\n",
    "        clap_expanded = torch.zeros(1, target_time_frames, embedding_dim)\n",
    "        projected_components['semantic'] = clap_expanded\n",
    "        print(f\"   âš ï¸ CLAP using zeros: {clap_expanded.shape}\")\n",
    "    \n",
    "    # Weighted fusion\n",
    "    fused_embeddings = (\n",
    "        fusion_weights['pitch'] * projected_components['pitch'] +\n",
    "        fusion_weights['encodec'] * projected_components['encodec'] +\n",
    "        fusion_weights['vq'] * projected_components['vq'] +\n",
    "        fusion_weights['semantic'] * projected_components['semantic']\n",
    "    )\n",
    "    \n",
    "    # Apply layer normalization\n",
    "    fused_embeddings = layer_norm(fused_embeddings)\n",
    "    \n",
    "    print(f\"\\nâœ… Fusion completed successfully\")\n",
    "    print(f\"   Fused embeddings shape: {fused_embeddings.shape}\")\n",
    "    print(f\"   Range: [{fused_embeddings.min():.3f}, {fused_embeddings.max():.3f}]\")\n",
    "    print(f\"   Mean: {fused_embeddings.mean():.3f}\")\n",
    "    print(f\"   Std: {fused_embeddings.std():.3f}\")\n",
    "    \n",
    "    # Visualize fusion results\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot individual component contributions\n",
    "    plt.subplot(3, 3, 1)\n",
    "    components_viz = {\n",
    "        'Pitch': projected_components['pitch'][0].detach().numpy(),\n",
    "        'Encodec': projected_components['encodec'][0].detach().numpy(),\n",
    "        'VQ-VAE': projected_components['vq'][0].detach().numpy(),\n",
    "        'CLAP': projected_components['semantic'][0].detach().numpy()\n",
    "    }\n",
    "    \n",
    "    for i, (name, component) in enumerate(components_viz.items()):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(component.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title(f'{name} Component (Weight: {list(fusion_weights.values())[i]})')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Embedding Dimension')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    # Plot fused embeddings\n",
    "    plt.subplot(3, 3, 5)\n",
    "    fused_viz = fused_embeddings[0].detach().numpy()\n",
    "    plt.imshow(fused_viz.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.title('Fused Embeddings (768 dims)')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Embedding Dimension')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot fusion statistics over time\n",
    "    plt.subplot(3, 3, 6)\n",
    "    fused_means = fused_embeddings[0].mean(dim=1).detach().numpy()\n",
    "    fused_stds = fused_embeddings[0].std(dim=1).detach().numpy()\n",
    "    frames = np.arange(len(fused_means))\n",
    "    plt.plot(frames, fused_means, label='Mean', alpha=0.7)\n",
    "    plt.plot(frames, fused_stds, label='Std', alpha=0.7)\n",
    "    plt.title('Fused Embedding Stats Over Time')\n",
    "    plt.xlabel('Time Frame')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot component contribution magnitudes\n",
    "    plt.subplot(3, 3, 7)\n",
    "    component_norms = {}\n",
    "    for name, component in components_viz.items():\n",
    "        component_norms[name] = np.linalg.norm(component, axis=1).mean()\n",
    "    \n",
    "    plt.bar(component_norms.keys(), component_norms.values(), alpha=0.7)\n",
    "    plt.title('Average Component Magnitudes')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot embedding distribution\n",
    "    plt.subplot(3, 3, 8)\n",
    "    all_fused = fused_embeddings.flatten().detach().numpy()\n",
    "    plt.hist(all_fused, bins=50, alpha=0.7, density=True)\n",
    "    plt.title('Fused Embedding Distribution')\n",
    "    plt.xlabel('Embedding Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot fusion weights\n",
    "    plt.subplot(3, 3, 9)\n",
    "    plt.pie(fusion_weights.values(), labels=fusion_weights.keys(), autopct='%1.1f%%')\n",
    "    plt.title('Fusion Weight Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Fusion failed: {e}\")\n",
    "    fused_embeddings = torch.zeros(1, 129, 768)\n",
    "    print(f\"   Using zero embeddings: {fused_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Audio Transcription Decoder\n",
    "\n",
    "Process fused embeddings through CRNN decoder to get pitch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STAGE 6: AUDIO TRANSCRIPTION DECODER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize decoder\n",
    "try:\n",
    "    decoder = EmbeddingValidationDecoder(\n",
    "        embedding_dim=768,\n",
    "        hidden_dim=256,\n",
    "        gru_hidden=128,\n",
    "        n_pitches=88,  # Piano range\n",
    "        dropout=0.2,\n",
    "        bidirectional=True\n",
    "    )\n",
    "    print(\"âœ… Audio decoder initialized successfully\")\n",
    "    print(f\"   Input dimension: 768\")\n",
    "    print(f\"   Hidden dimension: 256\")\n",
    "    print(f\"   GRU hidden: 128 (bidirectional)\")\n",
    "    print(f\"   Output pitches: 88 (A0-C8)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Decoder initialization failed: {e}\")\n",
    "    decoder = None\n",
    "\n",
    "if decoder is not None and fused_embeddings is not None:\n",
    "    print(\"\\nðŸ”„ Processing through audio decoder...\")\n",
    "    try:\n",
    "        decoder.eval()  # Set to eval mode\n",
    "        with torch.no_grad():\n",
    "            decoder_outputs = decoder(fused_embeddings)\n",
    "        \n",
    "        onset_logits = decoder_outputs['onset_logits']\n",
    "        frame_logits = decoder_outputs['frame_logits']\n",
    "        confidence_logits = decoder_outputs['confidence_logits']\n",
    "        onset_probs = decoder_outputs['onset_probs']\n",
    "        frame_probs = decoder_outputs['frame_probs']\n",
    "        confidence = decoder_outputs['confidence']\n",
    "        \n",
    "        print(f\"âœ… Decoder processing successful\")\n",
    "        print(f\"   Onset probabilities shape: {onset_probs.shape}\")\n",
    "        print(f\"   Frame probabilities shape: {frame_probs.shape}\")\n",
    "        print(f\"   Confidence shape: {confidence.shape}\")\n",
    "        print(f\"   Onset range: [{onset_probs.min():.3f}, {onset_probs.max():.3f}]\")\n",
    "        print(f\"   Frame range: [{frame_probs.min():.3f}, {frame_probs.max():.3f}]\")\n",
    "        print(f\"   Confidence range: [{confidence.min():.3f}, {confidence.max():.3f}]\")\n",
    "        print(f\"   Average onset activation: {onset_probs.mean():.3f}\")\n",
    "        print(f\"   Average frame activation: {frame_probs.mean():.3f}\")\n",
    "        print(f\"   Average confidence: {confidence.mean():.3f}\")\n",
    "        \n",
    "        # Analyze predictions\n",
    "        onset_threshold = 0.5\n",
    "        frame_threshold = 0.5\n",
    "        \n",
    "        onset_detections = (onset_probs > onset_threshold).sum()\n",
    "        frame_detections = (frame_probs > frame_threshold).sum()\n",
    "        high_confidence = (confidence > 0.7).sum()\n",
    "        \n",
    "        print(f\"   Onset detections (>{onset_threshold}): {onset_detections}\")\n",
    "        print(f\"   Frame detections (>{frame_threshold}): {frame_detections}\")\n",
    "        print(f\"   High confidence frames (>0.7): {high_confidence}\")\n",
    "        \n",
    "        # Guitar pitch range analysis (E2 to E6: MIDI 40-88, piano keys 19-67)\n",
    "        guitar_range_start = 19  # E2 in piano key index\n",
    "        guitar_range_end = 67    # E6 in piano key index\n",
    "        \n",
    "        guitar_onset_activity = onset_probs[0, :, guitar_range_start:guitar_range_end].mean()\n",
    "        guitar_frame_activity = frame_probs[0, :, guitar_range_start:guitar_range_end].mean()\n",
    "        \n",
    "        print(f\"   Guitar range onset activity: {guitar_onset_activity:.3f}\")\n",
    "        print(f\"   Guitar range frame activity: {guitar_frame_activity:.3f}\")\n",
    "        \n",
    "        # Visualize decoder outputs\n",
    "        plt.figure(figsize=(16, 14))\n",
    "        \n",
    "        # Plot onset probabilities piano roll\n",
    "        plt.subplot(3, 3, 1)\n",
    "        onset_viz = onset_probs[0].detach().numpy().T\n",
    "        plt.imshow(onset_viz, aspect='auto', origin='lower', cmap='hot', vmin=0, vmax=1)\n",
    "        plt.title('Onset Probabilities (88 Piano Keys)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Piano Key (A0-C8)')\n",
    "        # Mark guitar range\n",
    "        plt.axhline(guitar_range_start, color='cyan', linestyle='--', alpha=0.7, linewidth=1)\n",
    "        plt.axhline(guitar_range_end, color='cyan', linestyle='--', alpha=0.7, linewidth=1)\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot frame probabilities piano roll\n",
    "        plt.subplot(3, 3, 2)\n",
    "        frame_viz = frame_probs[0].detach().numpy().T\n",
    "        plt.imshow(frame_viz, aspect='auto', origin='lower', cmap='hot', vmin=0, vmax=1)\n",
    "        plt.title('Frame Probabilities (88 Piano Keys)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Piano Key (A0-C8)')\n",
    "        # Mark guitar range\n",
    "        plt.axhline(guitar_range_start, color='cyan', linestyle='--', alpha=0.7, linewidth=1)\n",
    "        plt.axhline(guitar_range_end, color='cyan', linestyle='--', alpha=0.7, linewidth=1)\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Plot confidence over time\n",
    "        plt.subplot(3, 3, 3)\n",
    "        confidence_viz = confidence[0].detach().numpy()\n",
    "        frames = np.arange(len(confidence_viz))\n",
    "        plt.plot(frames, confidence_viz, alpha=0.7)\n",
    "        plt.axhline(0.7, color='red', linestyle='--', alpha=0.5, label='High Confidence')\n",
    "        plt.title('Prediction Confidence Over Time')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot guitar range activity\n",
    "        plt.subplot(3, 3, 4)\n",
    "        guitar_onset = onset_probs[0, :, guitar_range_start:guitar_range_end].mean(dim=1).detach().numpy()\n",
    "        guitar_frame = frame_probs[0, :, guitar_range_start:guitar_range_end].mean(dim=1).detach().numpy()\n",
    "        plt.plot(frames, guitar_onset, label='Onset', alpha=0.7)\n",
    "        plt.plot(frames, guitar_frame, label='Frame', alpha=0.7)\n",
    "        plt.title('Guitar Range Activity (E2-E6)')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Average Probability')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot pitch activation distribution\n",
    "        plt.subplot(3, 3, 5)\n",
    "        pitch_activation = onset_probs[0].mean(dim=0).detach().numpy()\n",
    "        piano_keys = np.arange(88)\n",
    "        plt.plot(piano_keys, pitch_activation, alpha=0.7)\n",
    "        plt.axvspan(guitar_range_start, guitar_range_end, alpha=0.2, color='cyan', label='Guitar Range')\n",
    "        plt.title('Average Pitch Activation')\n",
    "        plt.xlabel('Piano Key Index')\n",
    "        plt.ylabel('Average Onset Probability')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot detection statistics\n",
    "        plt.subplot(3, 3, 6)\n",
    "        detection_stats = {\n",
    "            'Onset Det.': onset_detections.item(),\n",
    "            'Frame Det.': frame_detections.item(),\n",
    "            'High Conf.': high_confidence.item()\n",
    "        }\n",
    "        plt.bar(detection_stats.keys(), detection_stats.values(), alpha=0.7)\n",
    "        plt.title('Detection Statistics')\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot onset vs frame correlation\n",
    "        plt.subplot(3, 3, 7)\n",
    "        onset_flat = onset_probs.flatten().detach().numpy()\n",
    "        frame_flat = frame_probs.flatten().detach().numpy()\n",
    "        plt.scatter(onset_flat, frame_flat, alpha=0.1, s=1)\n",
    "        plt.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "        plt.title('Onset vs Frame Predictions')\n",
    "        plt.xlabel('Onset Probability')\n",
    "        plt.ylabel('Frame Probability')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot prediction distributions\n",
    "        plt.subplot(3, 3, 8)\n",
    "        plt.hist(onset_flat, bins=50, alpha=0.5, label='Onset', density=True)\n",
    "        plt.hist(frame_flat, bins=50, alpha=0.5, label='Frame', density=True)\n",
    "        plt.axvline(onset_threshold, color='blue', linestyle='--', alpha=0.7, label='Thresholds')\n",
    "        plt.axvline(frame_threshold, color='orange', linestyle='--', alpha=0.7)\n",
    "        plt.title('Prediction Distributions')\n",
    "        plt.xlabel('Probability')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot strongest activations\n",
    "        plt.subplot(3, 3, 9)\n",
    "        max_onset_per_frame = onset_probs[0].max(dim=1)[0].detach().numpy()\n",
    "        max_frame_per_frame = frame_probs[0].max(dim=1)[0].detach().numpy()\n",
    "        plt.plot(frames, max_onset_per_frame, label='Max Onset', alpha=0.7)\n",
    "        plt.plot(frames, max_frame_per_frame, label='Max Frame', alpha=0.7)\n",
    "        plt.title('Strongest Activations Over Time')\n",
    "        plt.xlabel('Time Frame')\n",
    "        plt.ylabel('Max Probability')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Decoder processing failed: {e}\")\n",
    "        onset_probs = None\n",
    "        frame_probs = None\n",
    "        confidence = None\n",
    "else:\n",
    "    onset_probs = None\n",
    "    frame_probs = None\n",
    "    confidence = None\n",
    "    print(\"âš ï¸ Skipping decoder - using random predictions for testing\")\n",
    "    onset_probs = torch.sigmoid(torch.randn(1, 129, 88))\n",
    "    frame_probs = torch.sigmoid(torch.randn(1, 129, 88))\n",
    "    confidence = torch.sigmoid(torch.randn(1, 129))\n",
    "    print(f\"   Random onset probs shape: {onset_probs.shape}\")\n",
    "    print(f\"   Random frame probs shape: {frame_probs.shape}\")\n",
    "    print(f\"   Random confidence shape: {confidence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Summary and Analysis\n",
    "\n",
    "Complete analysis of the entire pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE SUMMARY AND ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect pipeline statistics\n",
    "pipeline_stats = {\n",
    "    'input_audio': {\n",
    "        'shape': audio_batch.shape,\n",
    "        'duration_sec': audio_length,\n",
    "        'sample_rate': sample_rate,\n",
    "        'status': 'âœ…'\n",
    "    },\n",
    "    'basic_pitch': {\n",
    "        'shape': pitch_features.shape if pitch_features is not None else 'N/A',\n",
    "        'output_dim': 440 if pitch_features is not None else 'N/A',\n",
    "        'status': 'âœ…' if pitch_features is not None else 'âŒ'\n",
    "    },\n",
    "    'encodec': {\n",
    "        'embeddings_shape': encodec_embeddings.shape if encodec_embeddings is not None else 'N/A',\n",
    "        'codes_shape': encodec_codes.shape if encodec_codes is not None else 'N/A',\n",
    "        'status': 'âœ…' if encodec_embeddings is not None else 'âŒ'\n",
    "    },\n",
    "    'vq_vae': {\n",
    "        'embeddings_shape': vq_embeddings.shape if vq_embeddings is not None else 'N/A',\n",
    "        'indices_shape': vq_indices.shape if vq_indices is not None else 'N/A',\n",
    "        'commitment_loss': commitment_loss.item() if commitment_loss is not None else 'N/A',\n",
    "        'status': 'âœ…' if vq_embeddings is not None else 'âŒ'\n",
    "    },\n",
    "    'clap': {\n",
    "        'shape': clap_embeddings.shape if clap_embeddings is not None else 'N/A',\n",
    "        'model': 'laion/larger_clap_music' if clap_embeddings is not None else 'N/A',\n",
    "        'status': 'âœ…' if clap_embeddings is not None else 'âŒ'\n",
    "    },\n",
    "    'fusion': {\n",
    "        'shape': fused_embeddings.shape if fused_embeddings is not None else 'N/A',\n",
    "        'target_dim': 768,\n",
    "        'status': 'âœ…' if fused_embeddings is not None else 'âŒ'\n",
    "    },\n",
    "    'decoder': {\n",
    "        'onset_shape': onset_probs.shape if onset_probs is not None else 'N/A',\n",
    "        'frame_shape': frame_probs.shape if frame_probs is not None else 'N/A',\n",
    "        'status': 'âœ…' if onset_probs is not None else 'âŒ'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š COMPONENT STATUS:\")\n",
    "for component, stats in pipeline_stats.items():\n",
    "    status = stats.pop('status')\n",
    "    print(f\"\\n{status} {component.upper()}:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Overall pipeline health\n",
    "successful_components = sum(1 for stats in pipeline_stats.values() if 'âœ…' in str(stats))\n",
    "total_components = len(pipeline_stats)\n",
    "pipeline_health = successful_components / total_components\n",
    "\n",
    "print(f\"\\nðŸ¥ PIPELINE HEALTH: {pipeline_health:.1%} ({successful_components}/{total_components} components working)\")\n",
    "\n",
    "# Performance analysis\n",
    "if onset_probs is not None and frame_probs is not None:\n",
    "    print(\"\\nðŸŽµ TRANSCRIPTION ANALYSIS:\")\n",
    "    \n",
    "    # Note detection with thresholds\n",
    "    onset_threshold = 0.3\n",
    "    frame_threshold = 0.3\n",
    "    \n",
    "    onset_detections = (onset_probs > onset_threshold)\n",
    "    frame_detections = (frame_probs > frame_threshold)\n",
    "    \n",
    "    # Simple peak picking for notes\n",
    "    detected_notes = []\n",
    "    for frame_idx in range(onset_probs.shape[1]):\n",
    "        frame_onsets = onset_detections[0, frame_idx]\n",
    "        if frame_onsets.any():\n",
    "            active_pitches = torch.where(frame_onsets)[0]\n",
    "            for pitch in active_pitches:\n",
    "                # Convert piano key to MIDI note\n",
    "                midi_note = pitch.item() + 21  # A0 = 21\n",
    "                onset_prob = onset_probs[0, frame_idx, pitch].item()\n",
    "                frame_prob = frame_probs[0, frame_idx, pitch].item()\n",
    "                detected_notes.append({\n",
    "                    'frame': frame_idx,\n",
    "                    'time_sec': frame_idx * 0.023,  # ~23ms per frame\n",
    "                    'piano_key': pitch.item(),\n",
    "                    'midi_note': midi_note,\n",
    "                    'onset_prob': onset_prob,\n",
    "                    'frame_prob': frame_prob\n",
    "                })\n",
    "    \n",
    "    print(f\"   Notes detected (threshold {onset_threshold}): {len(detected_notes)}\")\n",
    "    \n",
    "    if detected_notes:\n",
    "        print(f\"   Time range: {detected_notes[0]['time_sec']:.2f}s - {detected_notes[-1]['time_sec']:.2f}s\")\n",
    "        \n",
    "        # Show top 10 strongest detections\n",
    "        top_notes = sorted(detected_notes, key=lambda x: x['onset_prob'], reverse=True)[:10]\n",
    "        print(f\"\\n   Top 10 strongest detections:\")\n",
    "        for i, note in enumerate(top_notes):\n",
    "            print(f\"     {i+1}. MIDI {note['midi_note']} at {note['time_sec']:.2f}s (onset: {note['onset_prob']:.3f})\")\n",
    "        \n",
    "        # Guitar range analysis\n",
    "        guitar_notes = [n for n in detected_notes if 40 <= n['midi_note'] <= 88]  # E2 to E6\n",
    "        print(f\"\\n   Notes in guitar range (E2-E6): {len(guitar_notes)}/{len(detected_notes)}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ No notes detected - try lowering threshold or check embeddings\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(\"\\nðŸ’¾ MEMORY USAGE ESTIMATION:\")\n",
    "total_memory_kb = 0\n",
    "\n",
    "if audio_batch is not None:\n",
    "    audio_kb = audio_batch.numel() * 4 / 1024  # 4 bytes per float32\n",
    "    total_memory_kb += audio_kb\n",
    "    print(f\"   Input audio: {audio_kb:.1f} KB\")\n",
    "\n",
    "if pitch_features is not None:\n",
    "    pitch_kb = pitch_features.numel() * 4 / 1024\n",
    "    total_memory_kb += pitch_kb\n",
    "    print(f\"   Basic Pitch features: {pitch_kb:.1f} KB\")\n",
    "\n",
    "if encodec_embeddings is not None:\n",
    "    encodec_kb = encodec_embeddings.numel() * 4 / 1024\n",
    "    total_memory_kb += encodec_kb\n",
    "    print(f\"   Encodec embeddings: {encodec_kb:.1f} KB\")\n",
    "\n",
    "if vq_embeddings is not None:\n",
    "    vq_kb = vq_embeddings.numel() * 4 / 1024\n",
    "    total_memory_kb += vq_kb\n",
    "    print(f\"   VQ-VAE embeddings: {vq_kb:.1f} KB\")\n",
    "\n",
    "if clap_embeddings is not None:\n",
    "    clap_kb = clap_embeddings.numel() * 4 / 1024\n",
    "    total_memory_kb += clap_kb\n",
    "    print(f\"   CLAP embeddings: {clap_kb:.1f} KB\")\n",
    "\n",
    "if fused_embeddings is not None:\n",
    "    fused_kb = fused_embeddings.numel() * 4 / 1024\n",
    "    total_memory_kb += fused_kb\n",
    "    print(f\"   Fused embeddings: {fused_kb:.1f} KB\")\n",
    "\n",
    "if onset_probs is not None:\n",
    "    predictions_kb = (onset_probs.numel() + frame_probs.numel() + confidence.numel()) * 4 / 1024\n",
    "    total_memory_kb += predictions_kb\n",
    "    print(f\"   Decoder predictions: {predictions_kb:.1f} KB\")\n",
    "\n",
    "print(f\"\\n   Total memory usage: {total_memory_kb:.1f} KB ({total_memory_kb/1024:.2f} MB)\")\n",
    "print(f\"   Memory per second: {total_memory_kb/audio_length:.1f} KB/sec\")\n",
    "\n",
    "# Next steps recommendations\n",
    "print(\"\\nðŸŽ¯ NEXT STEPS RECOMMENDATIONS:\")\n",
    "\n",
    "if pipeline_health == 1.0:\n",
    "    print(\"   âœ… All components working! Ready for training or real audio testing.\")\n",
    "    print(\"   â€¢ Try with real guitar audio files\")\n",
    "    print(\"   â€¢ Train the decoder on GuitarSet dataset\")\n",
    "    print(\"   â€¢ Implement note post-processing (frame-to-note)\")\n",
    "    print(\"   â€¢ Add tab assignment and technique detection\")\n",
    "elif pipeline_health >= 0.7:\n",
    "    print(\"   ðŸŸ¡ Most components working. Fix remaining issues:\")\n",
    "    failed_components = [comp for comp, stats in pipeline_stats.items() if 'âŒ' in str(stats)]\n",
    "    for comp in failed_components:\n",
    "        print(f\"   â€¢ Fix {comp} component\")\nelse:\n",
    "    print(\"   ðŸ”´ Multiple components failing. Priority fixes:\")\n",
    "    print(\"   â€¢ Check pre-trained model downloads\")\n",
    "    print(\"   â€¢ Verify dependencies (transformers, basic-pitch, encodec)\")\n",
    "    print(\"   â€¢ Test components individually\")\n",
    "\n",
    "if onset_probs is not None and len(detected_notes) == 0:\n",
    "    print(\"\\n   ðŸŽµ Transcription improvements:\")\n",
    "    print(\"   â€¢ Lower detection thresholds\")\n",
    "    print(\"   â€¢ Train decoder on labeled data\")\n",
    "    print(\"   â€¢ Check if embeddings contain musical information\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline debugging complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results for Further Analysis\n",
    "\n",
    "Save intermediate results for later analysis or debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save results\n",
    "save_results = False  # Set to True if you want to save\n",
    "\n",
    "if save_results:\n",
    "    print(\"ðŸ’¾ Saving pipeline results...\")\n",
    "    \n",
    "    results_dict = {\n",
    "        'pipeline_stats': pipeline_stats,\n",
    "        'audio_shape': audio_batch.shape,\n",
    "        'detected_notes': detected_notes if 'detected_notes' in locals() else [],\n",
    "        'pipeline_health': pipeline_health\n",
    "    }\n",
    "    \n",
    "    # Save embeddings if available\n",
    "    if fused_embeddings is not None:\n",
    "        results_dict['fused_embeddings'] = fused_embeddings.detach().numpy()\n",
    "    \n",
    "    if onset_probs is not None:\n",
    "        results_dict['onset_probs'] = onset_probs.detach().numpy()\n",
    "        results_dict['frame_probs'] = frame_probs.detach().numpy()\n",
    "    \n",
    "    # Save to numpy file\n",
    "    import os\n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "    np.savez('../results/pipeline_debug_results.npz', **results_dict)\n",
    "    print(\"   Results saved to ../results/pipeline_debug_results.npz\")\n",
    "else:\n",
    "    print(\"ðŸ’¾ Results not saved (set save_results=True to save)\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Debugging notebook complete!\")\n",
    "print(\"\\nUse this notebook to:\")\n",
    "print(\"â€¢ Test individual pipeline components\")\n",
    "print(\"â€¢ Analyze intermediate representations\")\n",
    "print(\"â€¢ Debug dimension mismatches\")\n",
    "print(\"â€¢ Visualize embedding quality\")\n",
    "print(\"â€¢ Monitor transcription performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}