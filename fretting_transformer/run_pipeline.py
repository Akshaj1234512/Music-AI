#!/usr/bin/env python3
"""
Unified Pipeline Runner for Fretting Transformer

Handles the complete pipeline: data preparation â†’ training â†’ evaluation
Can run individual stages or the full pipeline with a single command.
"""

import os
import argparse
import sys
import json
import shutil
from pathlib import Path
from datetime import datetime

# Add src to path for imports
sys.path.append('src')

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Run Fretting Transformer Pipeline')
    
    # Pipeline control
    parser.add_argument('--stage', type=str, 
                       choices=['data', 'train', 'eval', 'all'], 
                       default='all',
                       help='Which stage to run (data/train/eval/all)')
    
    # Output configuration
    parser.add_argument('--output_dir', type=str, 
                       default=None,
                       help='Base output directory (auto-generated if not specified)')
    parser.add_argument('--experiment_name', type=str,
                       default=None,
                       help='Experiment name (auto-generated if not specified)')
    
    # Data arguments
    parser.add_argument('--synthtab_path', type=str,
                       default='/data/andreaguz/SynthTab_Dev',
                       help='Path to SynthTab dataset')
    parser.add_argument('--data_category', type=str, default='jams',
                       choices=['jams', 'acoustic'],
                       help='Data category to use')
    parser.add_argument('--max_files', type=int, default=None,
                       help='Max files to process (for testing)')
    
    # Model configuration
    parser.add_argument('--model_type', type=str, default='paper',
                       choices=['paper', 'debug'],
                       help='Model size (paper=full, debug=small)')
    
    # Training configuration
    parser.add_argument('--num_epochs', type=int, default=100,
                       help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='Training batch size')
    parser.add_argument('--use_fp16', action='store_true',
                       help='Use mixed precision training')
    
    # GPU configuration
    parser.add_argument('--gpu_ids', type=str, default=None,
                       help='GPU IDs to use (e.g., "0" or "0,1")')
    parser.add_argument('--force_cpu', action='store_true',
                       help='Force CPU usage')
    
    # Evaluation configuration
    parser.add_argument('--apply_postprocessing', action='store_true',
                       help='Apply post-processing during evaluation')
    parser.add_argument('--compare_baseline', action='store_true',
                       help='Compare with baseline during evaluation')
    
    # System configuration
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed for reproducibility')
    parser.add_argument('--clean_start', action='store_true',
                       help='Remove existing output directory if it exists')
    
    return parser.parse_args()


def setup_gpu(gpu_ids=None, force_cpu=False):
    """Setup GPU configuration."""
    if force_cpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = ""
        print("ðŸ–¥ï¸  Forcing CPU usage")
        return
    
    if gpu_ids:
        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_ids
        print(f"ðŸ”§ Set CUDA_VISIBLE_DEVICES to: {gpu_ids}")
    
    # Check CUDA availability
    try:
        import torch
        if torch.cuda.is_available():
            print(f"ðŸš€ CUDA available with {torch.cuda.device_count()} GPU(s)")
            for i in range(torch.cuda.device_count()):
                name = torch.cuda.get_device_name(i)
                memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                print(f"   GPU {i}: {name} ({memory:.1f} GB)")
        else:
            print("ðŸ–¥ï¸  CUDA not available, using CPU")
    except ImportError:
        print("âš ï¸  PyTorch not installed, cannot check GPU status")


def create_experiment_dir(base_dir=None, experiment_name=None, clean_start=False):
    """Create experiment directory with timestamp."""
    if base_dir is None:
        base_dir = "experiments"
    
    if experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_name = f"run_{timestamp}"
    
    experiment_dir = os.path.join(base_dir, experiment_name)
    
    if clean_start and os.path.exists(experiment_dir):
        print(f"ðŸ—‘ï¸  Removing existing directory: {experiment_dir}")
        shutil.rmtree(experiment_dir)
    
    # Create directory structure
    subdirs = ['checkpoints', 'logs', 'data', 'evaluation']
    for subdir in subdirs:
        os.makedirs(os.path.join(experiment_dir, subdir), exist_ok=True)
    
    print(f"ðŸ“ Created experiment directory: {experiment_dir}")
    return experiment_dir


def run_data_preparation(args, experiment_dir):
    """Run data preparation stage using unified approach."""
    print("\n" + "="*50)
    print("ðŸ“Š STAGE 1: DATA PREPARATION")
    print("="*50)
    
    # With unified approach, data preparation is handled during training
    # Just create the data directory and validate the dataset path
    data_dir = os.path.join(experiment_dir, 'data')
    os.makedirs(data_dir, exist_ok=True)
    
    if not os.path.exists(args.synthtab_path):
        raise RuntimeError(f"SynthTab dataset not found at: {args.synthtab_path}")
    
    print(f"âœ“ SynthTab dataset found at: {args.synthtab_path}")
    print(f"âœ“ Using data category: {args.data_category}")
    if args.max_files:
        print(f"âœ“ Max files to process: {args.max_files}")
    else:
        print(f"âœ“ Processing all files")
    
    print("âœ… Data preparation setup completed")
    print("   (Actual data processing will happen during training with unified vocabulary)")
    
    return os.path.join(data_dir, 'unified_cache.pkl')


def run_training(args, experiment_dir, cache_path):
    """Run training stage using unified vocabulary approach."""
    print("\n" + "="*50)
    print("ðŸ‹ï¸  STAGE 2: TRAINING")
    print("="*50)
    
    # Use unified training approach directly instead of calling scripts
    import sys
    import os
    src_path = os.path.join(os.path.dirname(__file__), 'src')
    if src_path not in sys.path:
        sys.path.insert(0, src_path)
    
    import torch
    from torch.optim import AdamW
    from torch.amp import autocast, GradScaler
    from transformers import get_linear_schedule_with_warmup
    
    from data.unified_tokenizer import UnifiedFrettingTokenizer
    from data.unified_dataset import UnifiedFrettingDataProcessor, create_unified_data_loaders
    from model.unified_fretting_t5 import create_model_from_tokenizer
    
    def set_seed(seed):
        import random
        import numpy as np
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    print("=== Fretting Transformer Training ===")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name()}")
    
    # Setup
    set_seed(args.seed)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    checkpoints_dir = os.path.join(experiment_dir, 'checkpoints')
    os.makedirs(checkpoints_dir, exist_ok=True)
    
    # Save configuration
    config_path = os.path.join(checkpoints_dir, 'training_config.json')
    
    print("=== Data Loading and Processing ===")
    # Create unified tokenizer
    tokenizer = UnifiedFrettingTokenizer()
    print(f"Unified vocabulary size: {tokenizer.vocab_size}")
    
    # Load processed data using unified processor
    processor = UnifiedFrettingDataProcessor(synthtab_path=args.synthtab_path)
    processor.load_and_process_data(
        category=args.data_category,
        max_files=args.max_files,
        cache_path=os.path.join(experiment_dir, 'data', 'unified_cache.pkl')
    )
    
    if not processor.processed_sequences:
        raise RuntimeError("No sequences processed - check data path")
    
    # Create data splits
    train_dataset, val_dataset, test_dataset = processor.create_data_splits()
    train_loader, val_loader, test_loader = create_unified_data_loaders(
        train_dataset, val_dataset, test_dataset,
        batch_size=args.batch_size,
        num_workers=4
    )
    
    print("=== Model Creation ===")
    model = create_model_from_tokenizer(tokenizer, args.model_type)
    model_info = model.get_model_info()
    print(f"Model size: {model_info['parameters_millions']:.2f}M parameters")
    model.to(device)
    
    # Save config with model info
    config = {
        'args': vars(args),
        'tokenizer_info': tokenizer.get_vocab_info(),
        'model_info': model_info,
        'vocab_size': tokenizer.vocab_size,
        'architecture': 'unified_t5'
    }
    with open(config_path, 'w') as f:
        import json
        json.dump(config, f, indent=2)
    
    # Setup training
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
    total_steps = len(train_loader) * args.num_epochs // 4  # gradient_accumulation_steps = 4
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=total_steps)
    scaler = GradScaler() if args.use_fp16 else None
    
    print(f"=== Training ({args.num_epochs} epochs) ===")
    best_val_loss = float('inf')
    
    for epoch in range(args.num_epochs):
        print(f"\nEpoch {epoch + 1}/{args.num_epochs}")
        model.train()
        total_loss = 0
        
        for step, batch in enumerate(train_loader):
            batch = {k: v.to(device) for k, v in batch.items()}
            
            with autocast('cuda', enabled=args.use_fp16):
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    decoder_input_ids=batch.get('decoder_input_ids'),
                    labels=batch['labels']
                )
                loss = outputs.loss / 4  # gradient accumulation
            
            if args.use_fp16:
                scaler.scale(loss).backward()
            else:
                loss.backward()
            
            total_loss += loss.item() * 4
            
            if (step + 1) % 4 == 0:  # gradient accumulation
                if args.use_fp16:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
            
            if (step + 1) % 50 == 0:
                avg_loss = total_loss / (step + 1)
                print(f"  Step {step + 1}/{len(train_loader)}: Loss = {avg_loss:.4f}")
        
        avg_train_loss = total_loss / len(train_loader)
        print(f"  Training loss: {avg_train_loss:.4f}")
        
        # Validation
        if len(val_loader) > 0:
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(device) for k, v in batch.items()}
                    outputs = model(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        decoder_input_ids=batch.get('decoder_input_ids'),
                        labels=batch['labels']
                    )
                    val_loss += outputs.loss.item()
            
            val_loss /= len(val_loader)
            print(f"  Validation loss: {val_loss:.4f}")
            
            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_path = os.path.join(checkpoints_dir, 'best_model')
                model.model.save_pretrained(best_model_path)
                tokenizer.save(os.path.join(best_model_path, 'tokenizer.json'))
                print(f"  ðŸ’¾ Saved new best model (val_loss: {val_loss:.4f})")
        
        # Save checkpoint every 5 epochs
        if (epoch + 1) % 5 == 0:
            checkpoint_path = os.path.join(checkpoints_dir, f'checkpoint-epoch-{epoch + 1}')
            model.model.save_pretrained(checkpoint_path)
            tokenizer.save(os.path.join(checkpoint_path, 'tokenizer.json'))
            print(f"  ðŸ’¾ Saved checkpoint: {checkpoint_path}")
    
    print("âœ… Training completed successfully")
    
    # Find latest checkpoint
    checkpoint_dir = os.path.join(experiment_dir, 'checkpoints')
    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))
        return os.path.join(checkpoint_dir, latest_checkpoint)
    return None


def run_evaluation(args, experiment_dir, model_path, cache_path):
    """Run evaluation stage."""
    print("\n" + "="*50)
    print("ðŸ“Š STAGE 3: EVALUATION")
    print("="*50)
    
    from scripts.evaluate import main as eval_main
    
    # Override sys.argv for evaluation script
    eval_args = [
        'evaluate.py',
        '--model_path', model_path,
        '--synthtab_path', args.synthtab_path,
        '--data_category', args.data_category,
        '--cache_path', cache_path,
        '--batch_size', str(args.batch_size),
        '--output_dir', os.path.join(experiment_dir, 'evaluation')
    ]
    
    if args.max_files:
        eval_args.extend(['--max_files', str(args.max_files)])
    
    if args.apply_postprocessing:
        eval_args.append('--apply_postprocessing')
    
    if args.compare_baseline:
        eval_args.append('--compare_baseline')
    
    original_argv = sys.argv
    sys.argv = eval_args
    
    try:
        result = eval_main()
        if result != 0:
            raise RuntimeError("Evaluation failed")
        print("âœ… Evaluation completed successfully")
    finally:
        sys.argv = original_argv


def save_pipeline_config(args, experiment_dir):
    """Save pipeline configuration for reproducibility."""
    config = {
        'pipeline_args': vars(args),
        'timestamp': datetime.now().isoformat(),
        'command_line': ' '.join(sys.argv),
        'environment': {
            'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'not_set')
        }
    }
    
    config_path = os.path.join(experiment_dir, 'pipeline_config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"ðŸ’¾ Saved pipeline configuration to: {config_path}")


def print_final_summary(args, experiment_dir, success_stages):
    """Print final summary of the pipeline run."""
    print("\n" + "="*60)
    print("ðŸŽ¯ PIPELINE SUMMARY")
    print("="*60)
    
    print(f"ðŸ“ Experiment Directory: {experiment_dir}")
    print(f"âš™ï¸  Configuration: {args.model_type} model, {args.data_category} data")
    print(f"ðŸ“Š Data: {args.max_files or 'all'} files from {args.synthtab_path}")
    
    stage_names = {'data': 'Data Preparation', 'train': 'Training', 'eval': 'Evaluation'}
    print(f"\nðŸ“ˆ Completed Stages:")
    for stage in success_stages:
        print(f"   âœ… {stage_names.get(stage, stage)}")
    
    # Show key output files
    outputs = []
    if 'data' in success_stages:
        outputs.append(f"   ðŸ“Š Processed Data: {experiment_dir}/data/")
    if 'train' in success_stages:
        outputs.append(f"   ðŸ‹ï¸  Model Checkpoints: {experiment_dir}/checkpoints/")
        outputs.append(f"   ðŸ“ Training Logs: {experiment_dir}/logs/")
    if 'eval' in success_stages:
        outputs.append(f"   ðŸ“Š Evaluation Results: {experiment_dir}/evaluation/")
    
    if outputs:
        print(f"\nðŸ“‚ Key Outputs:")
        for output in outputs:
            print(output)
    
    print(f"\nðŸ”§ To reproduce this run:")
    print(f"   python run_pipeline.py --output_dir {experiment_dir} --experiment_name {os.path.basename(experiment_dir)}")


def main():
    """Main pipeline runner."""
    args = parse_args()
    
    print("ðŸŽ¸ Fretting Transformer Pipeline")
    print("="*60)
    
    # Setup GPU
    setup_gpu(args.gpu_ids, args.force_cpu)
    
    # Create experiment directory
    experiment_dir = create_experiment_dir(
        args.output_dir, 
        args.experiment_name, 
        args.clean_start
    )
    
    # Save configuration
    save_pipeline_config(args, experiment_dir)
    
    success_stages = []
    cache_path = None
    model_path = None
    
    try:
        # Stage 1: Data Preparation
        if args.stage in ['data', 'all']:
            cache_path = run_data_preparation(args, experiment_dir)
            success_stages.append('data')
        
        # Stage 2: Training
        if args.stage in ['train', 'all']:
            if cache_path is None:
                cache_path = os.path.join(experiment_dir, 'data', 'synthtab_cache.pkl')
            model_path = run_training(args, experiment_dir, cache_path)
            success_stages.append('train')
        
        # Stage 3: Evaluation
        if args.stage in ['eval', 'all']:
            if model_path is None:
                # Find existing checkpoint
                checkpoint_dir = os.path.join(experiment_dir, 'checkpoints')
                checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]
                if checkpoints:
                    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))
                    model_path = os.path.join(checkpoint_dir, latest_checkpoint)
            
            if model_path:
                run_evaluation(args, experiment_dir, model_path, cache_path)
                success_stages.append('eval')
            else:
                print("âš ï¸  No trained model found for evaluation")
        
        print_final_summary(args, experiment_dir, success_stages)
        print("\nðŸŽ‰ Pipeline completed successfully!")
        return 0
        
    except Exception as e:
        print(f"\nâŒ Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)